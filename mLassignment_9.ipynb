{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n"
      ],
      "metadata": {
        "id": "pXTJesDoQkBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8NmSQamiQe6o"
      },
      "outputs": [],
      "source": [
        "#Feature engineering is the process of preparing and transforming data to make it suitable for machine learning algorithms. It involves cleaning the data by handling missing values, outliers, and noise. Then, selecting the most important features and scaling numerical features to a similar range. Categorical variables are encoded into numerical form. Additional features may be constructed by combining or transforming existing ones. Time and date information can also be used to create relevant features. Overall, feature engineering helps improve the performance and accuracy of machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ":2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n"
      ],
      "metadata": {
        "id": "SdMnDCf1RTsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature selection is the process of selecting the most important features from a dataset to improve model performance and reduce overfitting. \n",
        "\n",
        "#Univariate Selection: Select features based on their individual relationship with the target variable using statistical tests.\n",
        "#Recursive Feature Elimination (RFE): Iteratively remove less important features based on their importance rankings.\n",
        "#L1 Regularization (Lasso): Apply a penalty to the model's cost function to eliminate irrelevant features.\n",
        "#Tree-based Methods: Rank features based on their importance in decision tree algorithms like Random Forest or Gradient Boosting.\n",
        "#Feature Importance with XGBoost: Use XGBoost algorithm to assign scores to features based on their contribution to model performance."
      ],
      "metadata": {
        "id": "rQUGfmywRM2w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n"
      ],
      "metadata": {
        "id": "pbirHiJDSKaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter and wrapper approaches are two common strategies for feature selection.\n",
        "\n",
        "                                #Filter approach:\n",
        "\n",
        "#Filter approach selects features based on their statistical properties or relevance to the target variable, independent of the machine learning algorithm.\n",
        "##It uses statistical measures like correlation, mutual information, or chi-square to evaluate the relationship between each feature and the target.\n",
        "#Pros: It is computationally efficient and can handle high-dimensional datasets. It provides a quick way to identify potentially relevant features.\n",
        "#Cons: It may overlook complex feature interactions. It does not consider the specific learning algorithm and may select irrelevant features.\n",
        "                                  \n",
        "                                #Wrapper approach:\n",
        "\n",
        "#Wrapper approach selects features by evaluating their performance with a specific machine learning algorithm through an iterative process.\n",
        "#It involves using a subset of features, training the model, and assessing its performance. Features are selected or eliminated based on the model's performance.\n",
        "#Pros: It takes into account the specific learning algorithm and can identify relevant features that interact well with the model.\n",
        "#Cons: It can be computationally expensive, especially for large datasets. It may be prone to overfitting if the evaluation metric is not carefully chosen. It may not generalize well to different algorithms.\n",
        "#scikit-learn provides implementations for both filter and wrapper approaches. For the filter approach, functions like SelectKBest and SelectPercentile can be used. For the wrapper approach, the RFECV (Recursive Feature Elimination with Cross-Validation) class can be employed. These approaches can be used in conjunction with various machine learning algorithms to perform feature selection efficiently."
      ],
      "metadata": {
        "id": "MxyFVV4XRsXv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Describe the overall feature selection process.\n",
        "\n",
        "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n"
      ],
      "metadata": {
        "id": "_EcwWbDISVbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "         #Define the objective.\n",
        "         #Preprocess the data.\n",
        "         #Explore the data.\n",
        "          #Choose a feature selection method.\n",
        "         #Apply the feature selection method.\n",
        "         #Set criteria for selecting features.\n",
        "         #Evaluate performance.\n",
        "          #Iterate and refine.\n",
        "          #Validate on test data.\n",
        "          #Monitor and update.\n",
        "#Key Principle of Feature Extraction:\n",
        "#Feature extraction aims to transform high-dimensional data into a lower-dimensional space while retaining important information. For example, Principal Component Analysis (PCA) finds new orthogonal features that capture the most variance in the data.\n",
        "# Algorithms:\n",
        "\n",
        "#PCA (Principal Component Analysis)\n",
        "#LDA (Linear Discriminant Analysis)\n",
        "#ICA (Independent Component Analysis)\n",
        "#NMF (Non-Negative Matrix Factorization)"
      ],
      "metadata": {
        "id": "3LrLBF0_SJOv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Describe the feature engineering process in the sense of a text categorization issue."
      ],
      "metadata": {
        "id": "szVr6D3WS-v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Preprocessing: Tokenization, lowercasing, stop word removal, stemming/lemmatization, and handling special characters.\n",
        "#Feature Extraction: Bag-of-Words, TF-IDF, word embeddings, N-grams, and POS tagging.\n",
        "#Feature Encoding: One-Hot Encoding, Label Encoding, and Word Embedding Encoding.\n",
        "#Feature Selection: Removing sparse features using statistical measures or model-based selection.\n",
        "#Dimensionality Reduction: PCA or LSA to reduce feature space dimensionality.\n",
        "#Feature Engineering: Adding custom features and domain-specific features.\n",
        "#Model Training and Evaluation: Using the engineered features to train and evaluate text categorization models.\n",
        "#The goal is to transform raw text data into meaningful numerical features that capture the important information for accurate text categorization."
      ],
      "metadata": {
        "id": "ovR_v6SeS7gZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
      ],
      "metadata": {
        "id": "kIaG8c1VUI57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Row 1: (2, 3, 2, 0, 2, 3, 3, 0, 1)\n",
        "#Row 2: (2, 1, 0, 0, 3, 2, 1, 3, 1)\n",
        "\n",
        "#The resemblance in cosine is calculated as follows:\n",
        "\n",
        "#Calculate the dot product of the two vectors:\n",
        "#dot_product = (2 * 2) + (3 * 1) + (2 * 0) + (0 * 0) + (2 * 3) + (3 * 2) + (3 * 1) + (0 * 3) + (1 * 1) = 22\n",
        "\n",
        "#Calculate the magnitude (Euclidean norm) of each vector:\n",
        "#magnitude_row1 = sqrt(2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2) = sqrt(39) ≈ 6.245\n",
        "#magnitude_row2 = sqrt(2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2) = sqrt(23) ≈ 4.796\n",
        "\n",
        "#Calculate the cosine similarity:\n",
        "#cosine_similarity = dot_product / (magnitude_row1 * magnitude_row2) = 22 / (6.245 * 4.796) ≈ 0.729\n",
        "\n",
        "#the resemblance in cosine between the two rows is approximately 0.729."
      ],
      "metadata": {
        "id": "S7Gh6wUHTqvL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
        "\n",
        "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
      ],
      "metadata": {
        "id": "yEdG8LSWU6J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hamming distance = Number of positions where the corresponding elements are different\n",
        "\n",
        "#To calculate the Hamming gap between 10001011 and 11001111:\n",
        "\n",
        "#Positions: 1 2 3 4 5 6 7 8\n",
        "#String 1: 1 0 0 0 1 0 1 1\n",
        "#String 2: 1 1 0 0 1 1 1 1\n",
        "\n",
        "#Hamming gap = 2 (Positions 2 and 6 have different elements)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#ii#Jaccard index = (Size of intersection) / (Size of union)\n",
        "\n",
        "#Given sets:\n",
        "#Set A: (1, 1, 0, 0, 1, 0, 1, 1)\n",
        "#Set B: (1, 0, 0, 1, 1, 0, 0, 1)\n",
        "\n",
        "#Size of intersection = 5 (Elements 1, 0, 0, 1, 1 are common)\n",
        "#Size of union = 8 (All elements combined)\n",
        "\n",
        "#Jaccard index = 5 / 8 = 0.625\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Similarity matching coefficient = (Number of matching elements) / (Size of set A)\n",
        "\n",
        "\n",
        "#Set A: (1, 1, 0, 0, 1, 0, 1, 1)\n",
        "#Set B: (1, 0, 0, 1, 1, 0, 0, 1)\n",
        "\n",
        "#Number of matching elements = 6 (Elements 1, 0, 0, 1, 1, 0 are common)\n",
        "#Size of set A = 8 (All elements in Set A)\n",
        "\n",
        "#Similarity matching coefficient = 6 / 8 = 0.75\n",
        "\n",
        "#the Jaccard index is 0.625 and the similarity matching coefficient is 0.75 for the given sets.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A5ciueaAU1O9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n"
      ],
      "metadata": {
        "id": "hT53d1ZDVioY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#High-dimensional data refers to data sets with a large number of features compared to the number of samples. Examples include image datasets, genomic data, and text data.\n",
        "\n",
        "#Difficulties in using machine learning on high-dimensional data include the curse of dimensionality, increased computational complexity, and increased sparsity.\n",
        "\n",
        "#To address these challenges, techniques like feature selection, dimensionality reduction (e.g., PCA), regularization, ensemble methods, and algorithm selection can be applied.\n",
        "\n",
        "#Python libraries like scikit-learn offer implementations of these techniques to handle high-dimensional data effectively.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2SC_3cV1VgYQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-dimensional data refers to data sets with a large number of features compared to the number of samples. Examples include image datasets, genomic data, and text data.\n",
        "\n",
        "Difficulties in using machine learning on high-dimensional data include the curse of dimensionality, increased computational complexity, and increased sparsity.\n",
        "\n",
        "To address these challenges, techniques like feature selection, dimensionality reduction (e.g., PCA), regularization, ensemble methods, and algorithm selection can be applied.\n",
        "\n",
        "Python libraries like scikit-learn offer implementations of these techniques to handle high-dimensional data effectively.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "65CywB0gV9Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#High-dimensional data sets have a large number of features compared to the number of samples. Examples include images, genomic data, and text data. Challenges with high-dimensional data include overfitting, increased computational complexity, and sparsity. Techniques like feature selection, dimensionality reduction, regularization, ensemble methods, and algorithm selection can help mitigate these challenges. Python libraries like scikit-learn provide tools to address high-dimensional data effectively."
      ],
      "metadata": {
        "id": "zhb6h-hpV6Gq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cVnhf1WiWFMA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}